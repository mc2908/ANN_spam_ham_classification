{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier\n",
    "## Assignment Preamble\n",
    "Please ensure you carefully read all of the details and instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please post on the forum or ask a tutor well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. These assignments are designed to help improve your understanding first and foremost – the process of doing the assignment is part of *learning*. They are also used to assess your ability, and so you must uphold academic integrity. Submitting plagiarised work risks your entire place on your degree.\n",
    "\n",
    "**The pass mark for this assignment is 40%.** We expect that students, on average, will be able to produce a submission which gets a mark between 50-70% within the normal workload allocation for the unit, but this will vary depending on individual backgrounds. Please ask for help if you are struggling.\n",
    "\n",
    "## Getting Started\n",
    "Spam refers to unwanted email, often in the form of advertisements. In the literature, an email that is **not** spam is called *ham*. Most email providers offer automatic spam filtering, where spam emails will be moved to a separate inbox based on their contents. Of course this requires being able to scan an email and determine whether it is spam or ham, a classification problem. This is the subject of this assignment.\n",
    "\n",
    "This assignment has two parts. Each part is worth 50% of the overall grade for this assignment.\n",
    "\n",
    "For part one you will write a supervised learning based classifier to determine whether a given email is spam or ham. You must write and submit the code in this notebook. The training data is provided for you. You may use any classification method. Marks will be awarded primarily based on the accuracy of your classifier on unseen test data, but there are also marks for estimating how accurate you think your classifier will be.\n",
    "\n",
    "In part two you will produce a short video explaining your implementation, any decisions or extensions you made, and what parameter values you used. This part is explained in more detail on the assignment page. The video file must be submitted with your assignment.\n",
    "\n",
    "### Choice of Algorithm\n",
    "While the classification method is a completely free choice, the assignment folder includes [a separate notebook file](data/naivebayes.ipynb) which can help you implement a Naïve Bayes solution. If you do use this notebook, you are still responsible for porting your code into *this* notebook for submission. A good implementation should give a high  enough accuracy to get a good grade on this section (50-70%).\n",
    "\n",
    "You could also consider a k-nearest neighbour algorithm, but this may be less accurate. Logistic regression is another option that you may wish to consider.\n",
    "\n",
    "If you are looking to go beyond the scope of the unit, you might be interested in building something more advanced, like an artificial neural network. This is possible just using `numpy`, but will require significant self-directed learning. *Extensions like this are left unguided and are not factored into the unit workload estimates.*\n",
    "\n",
    "**Note:** you may use helper functions in libraries like `numpy` or `scipy`, but you **must not** import code which builds entire models for you. This includes but is not limited to use of libraries like `scikit-learn`, `tensorflow`, or `pytorch` – there will be plenty of opportunities for these libraries in later units. The point of this assignment is to understand code the actual algorithm yourself. ***If you are in any doubt about any particular library or function please ask a tutor.*** Submissions which ignore this will receive penalties or even zero marks.\n",
    "\n",
    "If you choose to implement more than one algorithm, please feel free to include your code and talk about it in part two (your video presentation), but only the code in this notebook will be used in the automated testing.\n",
    "\n",
    "## Training Data\n",
    "The training data is described below and has 1000 rows. There is also a 500 row set of test data. These are functionally identical to the training data, they are just in a separate csv file to encourage you to split out your training and test data. You should consider how to best make use of all available data without overfitting, and to help produce an unbiased estimate for your classifier's accuracy.\n",
    "\n",
    "The cell below loads the training data into a variable called `training_spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_spam = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(np.int)\n",
    "print(\"Shape of the spam training data set:\", training_spam.shape)\n",
    "print(training_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your training set consists of 1000 rows and 55 columns. Each row corresponds to one email message. The first column is the _response_ variable and describes whether a message is spam `1` or ham `0`. The remaining 54 columns are _features_ that you will use to build a classifier. These features correspond to 54 different keywords (such as \"money\", \"free\", and \"receive\") and special characters (such as \":\", \"!\", and \"$\"). A feature has the value `1` if the keyword appears in the message and `0` otherwise.\n",
    "\n",
    "As mentioned there is also a 500 row set of *test data*. It contains the same 55 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(np.int)\n",
    "print(\"Shape of the spam testing data set:\", testing_spam.shape)\n",
    "print(testing_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One\n",
    "Write all of the code for your classifier below this cell. There is some very rough skeleton code in the cell directly below. You may insert more cells below this if you wish, but you must not duplicate any cells as this can break the grading script.\n",
    "\n",
    "### Submission Requirements\n",
    "Your code must provide a variable with the name `classifier`. This object must have a method called `predict` which takes input data and returns class predictions. The input will be a single $n \\times 54$ numpy array, your classifier should return a numpy array of length $n$ with classifications. There is a demo in the cell below, and a test you can run before submitting to check your code is working correctly.\n",
    "\n",
    "Your code must run on our test machine in under 30 seconds. If you wish to train a more complicated model (e.g. neural network) which will take longer, you are welcome to save the model's weights as a file and then load these in the cell below so we can test it. You must include the code which computes the original weights, but this must not run when we run the notebook – comment out the code which actually executes the routine and make sure it is clear what we need to change to get it to run. Remember that we will be testing your final classifier on additional hidden data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: training Score = [[0.24164788]]\n",
      "Epoch 400: training Score = [[0.22154441]]\n",
      "Epoch 600: training Score = [[0.21582629]]\n",
      "Epoch 800: training Score = [[0.21292771]]\n",
      "Epoch 1000: training Score = [[0.21108713]]\n",
      "Epoch 1200: training Score = [[0.20977297]]\n",
      "Epoch 1400: training Score = [[0.20876653]]\n",
      "Epoch 1600: training Score = [[0.20796166]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 25.984375 seconds\n",
      "Training cost = [[0.22617458]], Epochs = 318, Training Accuracy = 0.944, Validation Accuracy = 0.9233333333333333\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self,alpha=2,beta=10, type=\"LR\", net=[55, 55, 1], verbose=False):\n",
    "        self.verbose = verbose\n",
    "        self.theta = []\n",
    "        self.type = type\n",
    "        if type == \"LR\":\n",
    "            self.name = 'Logistic regression'\n",
    "            self.net = [0, 0, 0]\n",
    "        else:\n",
    "            self.name = 'Neural Net'\n",
    "            self.net = net\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.beta = beta    # regularization  factor  (10 best seen so far)\n",
    "        self.train_cost = 0\n",
    "\n",
    "    def train(self, data, data_eval):\n",
    "        Y = data[:, 0:1]  # labels (training set)\n",
    "        X = data[:, 1:]  # features only  (training set)\n",
    "        Y_eval = data_eval[:, 0:1]  # labels (evaluation set)\n",
    "        X_eval = data_eval[:, 1:]  # features only (evaluation set)\n",
    "        X_bias = np.concatenate((np.ones((np.shape(X)[0], 1)).astype(np.int8), X), axis=1)  # adding constant bias to the features\n",
    "        X_eval_bias = np.concatenate((np.ones((np.shape(X_eval)[0], 1)).astype(np.int8), X_eval), axis=1)  # adding constant bias to the features\n",
    "        t_start = time.process_time()\n",
    "        if self.type == 'LR':  # Logistic Regression\n",
    "            theta = np.zeros((np.shape(X_bias)[1], 1)).astype(np.float_)  # initialize weights + bias term\n",
    "            self.theta, train_cost,epochs = gradient_descent_LR(X_bias, theta, Y, self.alpha, self.beta, self.verbose)  # optimize the hyposis weight\n",
    "            accuracy = accuracy_score(self.predict_LR(X_bias, add_bias=False), Y)  # bias term has already been added\n",
    "            accuracy_eval = 0\n",
    "        elif self.type == 'NN':  # Neural Network\n",
    "            self.theta = self.Intialize_Theta() # initialize network weights\n",
    "            self.theta, train_cost, epochs = gradient_descent_NN(self, X_bias, Y, self.theta, X_eval_bias, Y_eval, self.verbose)  # optimize the network weight\n",
    "            accuracy = accuracy_score(self.predict_NN(X_bias, add_bias=False), Y) # calculate accuracy on the training data\n",
    "            accuracy_eval = accuracy_score(self.predict_NN(X_eval_bias, add_bias=False), Y_eval)  # calculate accurancy on the evaluation data\n",
    "        self.train_cost = train_cost\n",
    "        t_elapsed = time.process_time() - t_start\n",
    "        if self.verbose:\n",
    "            print(f\"Training time: {t_elapsed} seconds\")\n",
    "            print(f\"Training cost = {self.train_cost}, Epochs = {epochs}, Training Accuracy = {accuracy}, Validation Accuracy = {accuracy_eval}\")\n",
    "\n",
    "    def Intialize_Theta(self):\n",
    "        np.random.seed(1)\n",
    "        net = self.net\n",
    "        theta_vect_list = []\n",
    "        for i in range(len(net) - 1):\n",
    "            nParam = (net[i] + 1) * net[i + 1]\n",
    "            eps = np.sqrt(6) / np.sqrt(net[i] + net[i + 1])\n",
    "            #theta_vect_list.append(np.random.rand(nParam, 1) * 2 * eps - eps)\n",
    "            theta_vect_list.append(np.random.rand(nParam, 1) * 0.01)\n",
    "        theta_vect = np.concatenate(theta_vect_list, axis=0)\n",
    "        np.random.seed(None)\n",
    "        return theta_vect\n",
    "\n",
    "    def predict(self, data):\n",
    "        if self.type == \"LR\":\n",
    "            predictions = self.predict_LR(data)\n",
    "        else:\n",
    "            predictions = self.predict_NN(data)\n",
    "        predictions = predictions.reshape(max(predictions.shape))\n",
    "        return predictions\n",
    "\n",
    "    def predict_LR(self, data, add_bias=True):\n",
    "        # predict method for Linear Regression\n",
    "        if add_bias:\n",
    "            data = np.concatenate((np.ones((np.shape(data)[0], 1)).astype(np.int8), data), axis=1)\n",
    "        z_pred = np.matmul(data, self.theta)\n",
    "        y = sigmoid(z_pred)\n",
    "        predictions = np.zeros((y.shape[0], 1)).astype(np.int_)\n",
    "        idx_positive = y >= 0.5\n",
    "        predictions[idx_positive] = 1\n",
    "        return predictions\n",
    "\n",
    "    def predict_NN(self, data, theta=None, add_bias=True):\n",
    "        # predict method for Neural Networks\n",
    "        if theta is None:\n",
    "            theta = self.theta\n",
    "        # data must be without lables\n",
    "        m_theta = reshapeVect(theta, self.net)\n",
    "        if add_bias:\n",
    "            data = np.concatenate((np.ones((np.shape(data)[0], 1)).astype(np.int8), data), axis=1)\n",
    "        predictions = np.zeros((data.shape[0], 1)).astype(np.int_)\n",
    "        A, _ = forward_prop(data, m_theta)  # A = list of vectors of activation function values for each layer\n",
    "        pred = A[-1]\n",
    "        predictions = predictions + 1 * (pred.T >= 0.5)\n",
    "        return predictions\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        return self\n",
    "\n",
    "\n",
    "def gradient_descent_LR(X, theta, Y, alpha, beta, verbose=False):\n",
    "    # gradient descent for Lorgistic Regression\n",
    "    delta_cost = math.inf\n",
    "    cost = math.inf\n",
    "    max_err = 1e-8\n",
    "    iter_num = 0\n",
    "    cost_history = []\n",
    "    epoch_max = 2000\n",
    "    # for i in range(iter_max):\n",
    "    while delta_cost > max_err:\n",
    "    #while delta_cost > max_err:\n",
    "        iter_num += 1\n",
    "        new_cost, grad, train_cost = CostFunction_LR(X, theta, Y, beta)\n",
    "        # theta_i = theta_0 + alpha * d_J/d_theta\n",
    "        theta = theta - alpha * grad\n",
    "        delta_cost = abs(new_cost - cost)\n",
    "        cost = new_cost\n",
    "        cost_history.append(delta_cost)  # will comment out this before submitting as it slows down the alogorthm\n",
    "    if verbose:\n",
    "        print(f\"Number of iterations of Gradiant Descent = {iter_num}\")\n",
    "        plt.plot(range(iter_num), cost_history)\n",
    "        plt.show()\n",
    "    return theta, train_cost, iter_num\n",
    "\n",
    "\n",
    "def f1_score(pred,Y):\n",
    "    tp, tn, fp, fn = confusion_matrix(pred, Y)\n",
    "    f1 = tp / (tp + 0.5 * (fp + fn))\n",
    "    return f1\n",
    "\n",
    "\n",
    "def accuracy_score(pred,Y):\n",
    "    accuracy = np.sum(pred == Y)/len(pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def confusion_matrix(pred,Y):\n",
    "    yp = Y == 1\n",
    "    tp = np.sum(pred[yp] == Y[yp])\n",
    "    fn = np.sum(pred[yp] != Y[yp])\n",
    "    yn = Y == 0\n",
    "    tn = np.sum(pred[yn] == Y[yn])\n",
    "    fp = np.sum(pred[yn] != Y[yn])\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def gradient_descent_NN(model, X, Y, theta ,X_eval, Y_eval, verbose=False):\n",
    "    # gradient descent for Neural Net\n",
    "    validate_gradient = False\n",
    "    pq_max = 6\n",
    "    epoch = 0\n",
    "    cost_history = []\n",
    "    epoch_max = 2000\n",
    "    patience_factor = 50\n",
    "    # initiliaze the performance metric vectors\n",
    "    F1_train = np.array([])\n",
    "    F1_eval = np.array([])\n",
    "    ACC_eval = np.array([])\n",
    "    ACC_train = np.array([])\n",
    "    train_loss_vect = np.array([])\n",
    "    eval_loss_vect = np.array([])\n",
    "    gen_loss_vect = np.array([])\n",
    "    pq_vect = np.array([])\n",
    "    learn_slope_vect = np.array([])\n",
    "    \n",
    "    theta_buffer = theta\n",
    "    score_buffer = 0\n",
    "    epoch_buffer = 0\n",
    "    f1_eval_buffer = 0\n",
    "    k = 5\n",
    "    while epoch < epoch_max:\n",
    "        epoch += 1\n",
    "        if verbose and epoch % 200 == 0:\n",
    "            print(f\"Epoch {epoch}: training Score = {train_loss}\")\n",
    "        # calculate the cost function value and its gradient with respect the weights\n",
    "        new_cost, grad, train_loss = CostFunction_NN(X, theta, Y, model.beta, model.net)\n",
    "        # calculate the cost function value on the evaluation set\n",
    "        _, _, eval_loss = CostFunction_NN(X_eval, theta, Y_eval, model.beta, model.net)\n",
    "        # calculate performance metrics for training and validation set\n",
    "        train_loss_vect = np.append(train_loss_vect, train_loss[0])\n",
    "        eval_loss_vect = np.append(eval_loss_vect, eval_loss[0])\n",
    "        prediction_training = model.predict_NN(X, theta, add_bias=False)\n",
    "        f1_training = f1_score(prediction_training, Y)\n",
    "        F1_train = np.append(F1_train, f1_training)\n",
    "        prediction_eval = model.predict_NN(X_eval, theta, add_bias=False)\n",
    "        f1_eval = f1_score(prediction_eval, Y_eval)\n",
    "        F1_eval = np.append(F1_eval, f1_eval)\n",
    "        accuracy_eval = accuracy_score(prediction_eval, Y_eval)\n",
    "        accuracy_train = accuracy_score(prediction_training, Y)\n",
    "        ACC_eval = np.append(ACC_eval, accuracy_eval)\n",
    "        ACC_train = np.append(ACC_train, accuracy_train)\n",
    "        gen_loss_vect = np.append(gen_loss_vect, 0)\n",
    "        pq_vect= np.append(pq_vect, 0)\n",
    "        learn_slope_vect = np.append(learn_slope_vect, 1000)\n",
    "        if epoch > patience_factor:\n",
    "            # progress factor\n",
    "            learn_slope = (np.sum(train_loss_vect[epoch-k-1:epoch-1])/(k*min(train_loss_vect[epoch-k-1:epoch-1])) - 1) * 1000\n",
    "            learn_slope_vect[epoch-1] = learn_slope\n",
    "            # generalization loss\n",
    "            gloss = (eval_loss / min(eval_loss_vect) - 1) * 100\n",
    "            gen_loss_vect[epoch-1] = gloss\n",
    "            # generalization loss over progress\n",
    "            pq = gloss / learn_slope\n",
    "            pq_vect[epoch-1] = pq\n",
    "            if F1_eval[epoch-1] > f1_eval_buffer:\n",
    "                # store the weights which produce the best F1 score.\n",
    "                f1_eval_buffer = F1_eval[epoch-1]\n",
    "                theta_buffer = theta\n",
    "                score_buffer = train_loss\n",
    "                epoch_buffer = epoch\n",
    "            if pq > pq_max:  # stop the training process when the generalization loss over the progress is above 6%\n",
    "                break\n",
    "        if validate_gradient:\n",
    "            # run gradient cheking to make sure back prop works correctly\n",
    "            cost_fun = lambda w: CostFunction_NN(X, w, Y, model.beta, model.net)\n",
    "            check_gradient(theta, cost_fun, grad)\n",
    "        theta = theta - model.alpha * grad  # update theta\n",
    "        cost = new_cost\n",
    "        cost_history.append(cost)  # will comment out this before submitting as it slows down the alogorithm\n",
    "\n",
    "    theta_best = theta_buffer\n",
    "    epoch_best = epoch_buffer\n",
    "    train_cost_best = score_buffer\n",
    "    if verbose:\n",
    "        #  plot cost function trends for training and evaluation data\n",
    "        plt.plot(np.array(range(F1_eval.shape[0])), train_loss_vect)\n",
    "        plt.plot(np.array(range(F1_eval.shape[0])), eval_loss_vect)\n",
    "        plt.title(\"Cost Function\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"loss value\")\n",
    "        plt.show()\n",
    "        # plot the trend of the PQ term\n",
    "        #plt.plot(np.array(range(F1_eval.shape[0])), gen_loss_vect)\n",
    "        plt.plot(np.array(range(F1_eval.shape[0])), pq_vect)\n",
    "        plt.title(\"GenLoss over progress\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"value\")\n",
    "        plt.show()\n",
    "        # plot the trend of the F1 score\n",
    "        plt.plot(np.array(range(F1_eval.shape[0])), F1_train)\n",
    "        plt.plot(np.array(range(F1_eval.shape[0])), F1_eval)\n",
    "        plt.title(\"F1 score\")\n",
    "        plt.xlabel(\"epochs\")\n",
    "        plt.ylabel(\"value\")\n",
    "        plt.show()\n",
    "    return theta_best, train_cost_best, epoch_best\n",
    "\n",
    "\n",
    "def CostFunction_LR(X, theta, Y, beta=1):\n",
    "    # X = m x n+1 matrix where m  = number of training examples and n is the number of feature\n",
    "    # theta = n+1 x 1 vector\n",
    "    # Y =  m x 1 vector\n",
    "    m = np.shape(X)[0]\n",
    "    L = np.eye(theta.shape[0])\n",
    "    L[0, 0] = 0\n",
    "    theta_reg = theta\n",
    "    theta_reg[0, 0] = 0\n",
    "    z = np.matmul(X, theta)\n",
    "    cost = -1 / m * np.sum(+np.log(sigmoid(z)) * Y + np.log(1-sigmoid(z)) * (1-Y)) + beta/2/ m * np.sum(np.power(theta_reg, 2))  # this is a scalar\n",
    "    train_cost = -1 / m * np.sum(+np.log(sigmoid(z)) * Y + np.log(1-sigmoid(z)) * (1-Y))\n",
    "    # the gradiant must be a vector of size (n+1) x 1\n",
    "    grad = 1 / m * np.matmul(X.T, sigmoid(z) - Y) + beta/m * np.matmul(L, theta)\n",
    "    return cost, grad, train_cost\n",
    "\n",
    "\n",
    "def CostFunction_NN(X, thetaVect, Y, beta, net):\n",
    "    m_theta = reshapeVect(thetaVect, net)\n",
    "    m = X.shape[0]\n",
    "    layers = len(net)\n",
    "    D_list = [np.zeros(theta.shape) for theta in m_theta]\n",
    "    # forward prop (also computes the cost)\n",
    "    A, Z = forward_prop(X, m_theta)   # A = list of vectors of activation function values for each layer\n",
    "    h = A[-1]  # output prediction\n",
    "    cost = -np.log(h) @ Y - np.log(1-h) @ (1-Y)  # cross entropy loss\n",
    "    # back prop()\n",
    "    delta = back_prop(Y, A, m_theta)\n",
    "    # update deltas\n",
    "    for i in range(layers):\n",
    "        if i == layers - 1:\n",
    "            break\n",
    "        # calculating the partial derivatives of the cost function with respects the net weights\n",
    "        D_list[i] = D_list[i] + A[i] @ delta[i+1][1:].T\n",
    "    train_cost = cost\n",
    "    for i in range(len(m_theta)):\n",
    "        I = np.eye(m_theta[i].shape[0])\n",
    "        I[0, 0] = 0\n",
    "        D_list[i] = 1/m * (D_list[i] + beta * (I @ m_theta[i]))  # gradients regularization term\n",
    "        cost += beta/2 * np.sum((I @ np.power(m_theta[i], 2)) @ np.ones((m_theta[i].shape[1], 1)))  # cost function regularization term\n",
    "    train_cost = train_cost/m\n",
    "    cost = cost / m\n",
    "    D_list_flat = [np.reshape(d, (d.shape[0] * d.shape[1], 1)) for d in D_list]\n",
    "    grad_vect = np.concatenate(D_list_flat, axis=0)  # flatten the gradiant matrices to one vector\n",
    "    return cost, grad_vect, train_cost\n",
    "\n",
    "\n",
    "def reshapeVect(thetaVect,net):\n",
    "    #  Reshape the gradients vector into L-1 matrices of size  n(ly)+1 x n(ly+1) where n(lY) is the mumber of neurons in layer ly\n",
    "    point = 0\n",
    "    m_theta = [[] for _ in range(len(net)-1)]\n",
    "    for lyr, neurons in enumerate(net):\n",
    "        if lyr == len(net)-1:\n",
    "            continue\n",
    "        next_lyr_neurons = net[lyr+1]\n",
    "        elements = (neurons+1) * next_lyr_neurons\n",
    "        m_theta[lyr] = np.reshape(thetaVect[point : point+elements], (neurons+1, next_lyr_neurons))  # +1 to take care of the bias\n",
    "        point += elements\n",
    "    return m_theta\n",
    "\n",
    "\n",
    "def forward_prop(X, m_theta):\n",
    "    # Forward propagation: calculates the values of the activation function for each neurons in each layer\n",
    "    layers = len(m_theta) + 1\n",
    "    A = [[] for _ in range(layers)]\n",
    "    Z = [[] for _ in range(layers)]\n",
    "    A[0] = X.T\n",
    "    for lyr_num in range(1, layers):\n",
    "        Z[lyr_num] = np.matmul(m_theta[lyr_num-1].T, A[lyr_num-1])\n",
    "        A[lyr_num] = sigmoid(Z[lyr_num])\n",
    "        if lyr_num < layers-1:  # add the bias term to al layers except the last\n",
    "            A[lyr_num] = np.insert(A[lyr_num], 0, 1, axis=0)\n",
    "    return A, Z\n",
    "\n",
    "\n",
    "def back_prop(Y, A, m_theta):\n",
    "    # Back propagation: calculate the \"errors\" in each layer\n",
    "    layers = len(m_theta)+1\n",
    "    delta = [[] for _ in range(layers)]\n",
    "    delta[-1] = np.insert(A[layers-1] - Y.T, 0, 1, axis=0)\n",
    "    for lyr_num in reversed(range(layers-1)):\n",
    "        delta_no_bias = delta[lyr_num+1][1:]  # remove the bias\n",
    "        delta[lyr_num] = np.matmul(m_theta[lyr_num], delta_no_bias) * A[lyr_num]*(1-A[lyr_num])\n",
    "    return delta\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Neurons activation function\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "\n",
    "def check_gradient(theta, LossFun, grad):\n",
    "    # compare the gradiant calculated by means of forward and back prop with respect to a numerical grediant.\n",
    "    # this is to check that the implementation of forward and back prop is correct\n",
    "    theta_new = theta.copy\n",
    "    numgrad = np.zeros(theta.shape)\n",
    "    perturb = np.zeros(theta.shape)\n",
    "    eps = 1e-7\n",
    "    for p in range(len(theta)):\n",
    "        perturb[p] = eps\n",
    "        loss1, _, _ = LossFun(theta - perturb)\n",
    "        loss2, _, _ = LossFun(theta + perturb)\n",
    "        # Compute Numerical Gradient\n",
    "        numgrad[p] = (loss2 - loss1) / (2 * eps)\n",
    "        perturb[p] = 0\n",
    "        deltagrad = numgrad[p] - grad[p]\n",
    "        if deltagrad > 1e-7:\n",
    "            print(\"Gradient is Wrong\")\n",
    "        print(f\"Idx = {p}, grad = {grad[p]}, numeric grad = {numgrad[p]}. Difference = {grad[p] - numgrad[p]}\")\n",
    "    return numgrad\n",
    "\n",
    "\n",
    "def repeated_k_fold(models, data, k=5, n=20, verbose=False):\n",
    "    # repeate k-fold cross validation n time to have a better understanding of the performance distribution\n",
    "    scores_n_runs = np.array([[] for _ in range(len(models))]).reshape(0, len(models))\n",
    "    for i in range(n):\n",
    "        np.random.shuffle(data)\n",
    "        _, _, scores_1run = k_fold_validation(models, data, k, verbose)\n",
    "        scores_1_run = np.array(scores_1run).reshape(1, len(scores_1run))\n",
    "        scores_n_runs = np.concatenate([scores_n_runs, scores_1_run], axis=0)\n",
    "    plt.boxplot(scores_n_runs, showmeans=True, vert=False)\n",
    "    plt.title(\"Models Comparison\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Models\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def k_fold_validation(models, data, k=5, verbose=False):\n",
    "    \n",
    "    training_sets = np.split(data, k)\n",
    "    r = set(range(k))\n",
    "    scores = []\n",
    "    losses = []\n",
    "    model_identifiers = []\n",
    "    print(f\"K-fold Cross validation. k = {k}\")\n",
    "    # loop through the models\n",
    "    for num, model in enumerate(models):\n",
    "        if verbose:\n",
    "            print(f\"Evaluating {model.name}, Network architecture = {model.net}. Parameters: alpha = {model.alpha}, beta = {model.beta}\")\n",
    "        tot_score = 0\n",
    "        tot_loss = 0\n",
    "        n_iter = 0\n",
    "        # loop through all combinations k-2 folds.\n",
    "        # k-2 folds for training\n",
    "        # 1 fold for evaluation (early stopping only for NN)\n",
    "        # 1 fold for testing the net model performances\n",
    "        for idx, comb in enumerate(itertools.combinations(range(k), k-1-1)):  # -1 for validation data and -1 for test data\n",
    "            n_iter += 1\n",
    "            train_data_list = []\n",
    "            for i in comb:\n",
    "                train_data_list.append(training_sets[i])\n",
    "            train_data = np.concatenate(train_data_list)\n",
    "            rem = tuple(r - set(comb))\n",
    "            val_data = training_sets[rem[0]]\n",
    "            test_data = training_sets[rem[1]]\n",
    "            model.train(train_data, val_data)  # validation data are used for early stopping\n",
    "            score, _, eval_loss = validate(model, test_data) # test data are used to determine the net perfomances\n",
    "            tot_score += score\n",
    "            tot_loss += eval_loss\n",
    "            if verbose:\n",
    "                print(f\"{model.name} Training number {idx}, Test score = {score}, Test loss= {eval_loss}\")\n",
    "        avg_score = tot_score / n_iter\n",
    "        avg_loss = tot_loss / n_iter\n",
    "        scores.append(avg_score)\n",
    "        losses.append(-avg_loss)  # since I am taking the maximum value I need to change sign\n",
    "        if verbose:\n",
    "            if model.type == \"NN\":\n",
    "                model_identifiers.append(model.type + \", alpha = \" + str(model.alpha) + \", beta =\" + str(model.beta) + \", Net config =\" + str(model.net))\n",
    "            else:\n",
    "                model_identifiers.append(model.type + \", alpha = \" + str(model.alpha) + \", beta =\" + str(model.beta))\n",
    "            print(f\"{model.name}, Average score = {avg_score}, Average evaluation loss {avg_loss}\")\n",
    "            print()\n",
    "    score_classifier = list(zip(scores, losses, models))\n",
    "    # get the model which has the best average score (accuracy)\n",
    "    best_score, min_loss, best_classifier = max(score_classifier)\n",
    "    if verbose:\n",
    "        plt.barh(model_identifiers, scores)\n",
    "        plt.title(\"Average score model comparison\")\n",
    "        plt.xlabel(\"Average score\")\n",
    "        plt.xlim(0.90,0.94)\n",
    "        plt.show()\n",
    "        if model.type == \"LR\":\n",
    "            print(f\"Best classifier: {best_classifier.name}. Parameters: alpha = {best_classifier.alpha}, beta = {best_classifier.beta}. Average score = {best_score}, Average loss = {-min_loss}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Best classifier: {best_classifier.name}, Network architecture = {best_classifier.net}. Parameters: alpha = {best_classifier.alpha}, beta = {best_classifier.beta}. Average score = {best_score}, Average loss = {-min_loss}\")\n",
    "    return best_classifier, avg_score, scores,\n",
    "\n",
    "\n",
    "def validate(model, data, verbose=False):\n",
    "    # get the model performances of the validation/ test data\n",
    "    Y = data[:, 0]\n",
    "    data_no_label = data[:, 1:]\n",
    "    y = model.predict(data_no_label)\n",
    "    if model.type == \"NN\":\n",
    "        loss, _, eval_cost = CostFunction_NN(np.concatenate((np.ones((np.shape(data_no_label)[0],1)).astype(np.int8),data_no_label),axis=1) , model.theta, Y, model.beta,model.net)\n",
    "    else:\n",
    "        loss, _, eval_cost = CostFunction_LR(np.concatenate((np.ones((np.shape(data_no_label)[0], 1)).astype(np.int8), data_no_label), axis=1),model.theta, Y, model.beta)\n",
    "    avg_score = np.sum(y == Y) / y.shape[0]\n",
    "    if verbose:\n",
    "        print(f\"Train Cost = {model.train_cost}, Test cost = {eval_cost}, Test Accuracy = {avg_score}\")\n",
    "    return avg_score, loss, eval_cost\n",
    "\n",
    "\n",
    "def performance_distribtion(data, model, nS):\n",
    "    # evaluates the performance of the selected model and ptot the histogram\n",
    "    classifiers = [model]\n",
    "    scores = np.zeros((nS, 1))\n",
    "    for i in range(nS):\n",
    "        np.random.shuffle(data)\n",
    "        _, avg_score,_ = k_fold_validation(classifiers, data, k=5, verbose=False)\n",
    "        print(f\"Iteration {i+1} of {nS}, Average Score = {avg_score}\")\n",
    "        scores[i] = avg_score\n",
    "    title = \"Model Name = \" + model.name + \", alpha = \" + str(model.alpha) + \", beta =\" + str(model.beta) + \", Network config =\" + str(model.net)\n",
    "    plt.hist(scores, bins=20, density=False)\n",
    "    plt.title = title\n",
    "    plt.xlabel = \"k-fold score\"\n",
    "    plt.ylabel = \"Occurrences\"\n",
    "    plt.show()\n",
    "\n",
    "    dstr_mean = np.average(scores)\n",
    "    dstr_std = np.std(scores)\n",
    "    dstr_median = np.median(scores)\n",
    "    min_val = min(scores)\n",
    "    max_val = max(scores)\n",
    "    print(f\"Distribution parameters: Mean = {dstr_mean} Std = {dstr_std}, Median = {dstr_median}, Maximum = {max_val}, Minimum = {min_val}\")\n",
    "    return dstr_mean, dstr_std, dstr_median\n",
    "\n",
    "def create_classifier():\n",
    "    train_data = np.loadtxt(open(\"data/training_spam.csv\"), delimiter=\",\").astype(np.int8)\n",
    "    additional_data = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(np.int8)\n",
    "    np.random.shuffle(additional_data)\n",
    "    np.random.shuffle(train_data)\n",
    "    validation_data = additional_data[:300] # 3/5 of the test data are used to monitor the network generalization error and stop the net training.\n",
    "    classifier = SpamClassifier(2, 1, 'NN', net=[54, 28, 28, 1], verbose=True)  # Neural Network classifier\n",
    "    #classifier = SpamClassifier(4, 2, 'LR', net=[], verbose=False)  # Logistic regression classifier\n",
    "    classifier.train(train_data,validation_data)\n",
    "    return classifier\n",
    "\n",
    "classifier = create_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Estimate\n",
    "In the cell below there is a function called `my_accuracy_estimate()` which returns `0.5`. Before you submit the assignment, write your best guess for the accuracy of your classifier into this function, as a percentage between `0` and `1`. So if you think you will get 80% of inputs correct, return the value `0.8`. This will form a small part of the marking criteria for the assignment, to encourage you to test your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_accuracy_estimate():\n",
    "    return 0.925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write all of the code for your classifier above this cell.\n",
    "\n",
    "### Testing Details\n",
    "Your classifier will be tested against some hidden data from the same source as the original. The accuracy (percentage of classifications correct) will be calculated, then benchmarked against common methods. At the very high end of the grading scale, your accuracy will also be compared to the best submissions from other students (in your own cohort and others!). Your estimate from the cell above will also factor in, and you will be rewarded for being close to your actual accuracy (overestimates and underestimates will be treated the same).\n",
    "\n",
    "#### Test Cell\n",
    "The following code will run your classifier against the provided test data. To enable it, set the constant `SKIP_TESTS` to `False`.\n",
    "\n",
    "The original skeleton code above classifies every row as ham, but once you have written your own classifier you can run this cell again to test it. So long as your code sets up a variable called `classifier` with a method called `predict`, the test code will be able to run. \n",
    "\n",
    "Of course you may wish to test your classifier in additional ways, but you *must* ensure this version still runs before submitting.\n",
    "\n",
    "**IMPORTANT**: you must set `SKIP_TESTS` back to `True` before submitting this file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data is: 0.928\n"
     ]
    }
   ],
   "source": [
    "SKIP_TESTS = False\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    testing_spam = np.loadtxt(open(\"data/testing_spam.csv\"), delimiter=\",\").astype(np.int)\n",
    "    test_data = testing_spam[:, 1:]\n",
    "    test_labels = testing_spam[:, 0]\n",
    "    predictions = classifier.predict(test_data)\n",
    "    accuracy = np.count_nonzero(predictions == test_labels)/test_labels.shape[0]\n",
    "    print(f\"Accuracy on test data is: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59d6bceb43ad854b001cc67cf0fc07f9",
     "grade": false,
     "grade_id": "cell-ce83a675162843d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must set the SKIP_TESTS constant to True in the cell above.\n",
      "INFO: Make sure you follow the instructions on the assignment page to submit your video.\n",
      "Failing to include this could result in an overall grade of zero for both parts.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your submission is not ready! Please read and follow the instructions above."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_TESTS constant to True in the cell above.\")\n",
    "    \n",
    "p3 = pathlib.Path('./spamclassifier.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"This notebook file must be named spamclassifier.ipynb\")\n",
    "    \n",
    "if \"create_classifier\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"You must include a function called create_classifier.\")\n",
    "\n",
    "if \"my_accuracy_estimate\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"You must include a function called my_accuracy_estimate.\")\n",
    "else:\n",
    "    if my_accuracy_estimate() == 0.5:\n",
    "        print(\"Warning:\")\n",
    "        print(\"You do not seem to have provided an accuracy estimate, it is set to 0.5.\")\n",
    "        print(\"This is the actually the worst possible accuracy – if your classifier\")\n",
    "        print(\"got 0.1 then it could invert its results to get 0.9!\")\n",
    "    \n",
    "print(\"INFO: Make sure you follow the instructions on the assignment page to submit your video.\")\n",
    "print(\"Failing to include this could result in an overall grade of zero for both parts.\")\n",
    "print()\n",
    "\n",
    "if fail:\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print(\"All checks passed. When you are ready to submit, upload the notebook and readme file to the\")\n",
    "    print(\"assignment page, without changing any filenames.\")\n",
    "    print()\n",
    "    print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "badbc892f539e03ad0acdb369f7e0993",
     "grade": true,
     "grade_id": "cell-b64bc40ab6485b50",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a test cell. Please do not modify or delete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
